"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[921],{3905:(e,t,n)=>{n.d(t,{Zo:()=>c,kt:()=>d});var a=n(67294);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,r=function(e,t){if(null==e)return{};var n,a,r={},i=Object.keys(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||(r[n]=e[n]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(a=0;a<i.length;a++)n=i[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(r[n]=e[n])}return r}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},c=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},h="mdxType",m={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},u=a.forwardRef((function(e,t){var n=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),h=p(n),u=r,d=h["".concat(l,".").concat(u)]||h[u]||m[u]||i;return n?a.createElement(d,o(o({ref:t},c),{},{components:n})):a.createElement(d,o({ref:t},c))}));function d(e,t){var n=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=n.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s[h]="string"==typeof e?e:r,o[1]=s;for(var p=2;p<i;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}u.displayName="MDXCreateElement"},53717:(e,t,n)=>{n.r(t),n.d(t,{assets:()=>l,contentTitle:()=>o,default:()=>m,frontMatter:()=>i,metadata:()=>s,toc:()=>p});var a=n(87462),r=(n(67294),n(3905));const i={slug:"chainlit-langchain-prem",title:"Talk to your Data with ChainLit and Langchain",authors:["tiero","filippopedrazzinfp"],tags:["llm","self-hosted","prem","open-source","langchain","chainlit","vicuna-7b","chroma","vector-store"]},o=void 0,s={permalink:"/blog/chainlit-langchain-prem",editUrl:"https://github.com/premAI-io/dev-portal/blog/2023-07-05-chainlit-langchain-qa/index.md",source:"@site/blog/2023-07-05-chainlit-langchain-qa/index.md",title:"Talk to your Data with ChainLit and Langchain",description:"Build a chatbot that talks to your data with Prem using LangChain, Chainlit, Chroma Vector Store and Vicuna 7B model, self-hosted on your MacOS laptop.",date:"2023-07-05T00:00:00.000Z",formattedDate:"July 5, 2023",tags:[{label:"llm",permalink:"/blog/tags/llm"},{label:"self-hosted",permalink:"/blog/tags/self-hosted"},{label:"prem",permalink:"/blog/tags/prem"},{label:"open-source",permalink:"/blog/tags/open-source"},{label:"langchain",permalink:"/blog/tags/langchain"},{label:"chainlit",permalink:"/blog/tags/chainlit"},{label:"vicuna-7b",permalink:"/blog/tags/vicuna-7-b"},{label:"chroma",permalink:"/blog/tags/chroma"},{label:"vector-store",permalink:"/blog/tags/vector-store"}],readingTime:3.54,hasTruncateMarker:!0,authors:[{name:"Marco Argentieri",title:"Bitcoin wizard",url:"https://github.com/tiero",imageURL:"https://github.com/tiero.png",key:"tiero"},{name:"Filippo Pedrazzini",title:"Core contributor @ PremAI",url:"https://github.com/filopedraz",imageURL:"https://github.com/filopedraz.png",key:"filippopedrazzinfp"}],frontMatter:{slug:"chainlit-langchain-prem",title:"Talk to your Data with ChainLit and Langchain",authors:["tiero","filippopedrazzinfp"],tags:["llm","self-hosted","prem","open-source","langchain","chainlit","vicuna-7b","chroma","vector-store"]},nextItem:{title:"Serving Falcon 7B Instruct with FastAPI and Docker",permalink:"/blog/serving-falcon-7b-fastapi-docker"}},l={authorsImageUrls:[void 0,void 0]},p=[{value:"What is ChainLit?",id:"what-is-chainlit",level:3},{value:"What is Langchain?",id:"what-is-langchain",level:3},{value:"What is Prem?",id:"what-is-prem",level:3},{value:"Talk to your data with Prem",id:"talk-to-your-data-with-prem",level:2},{value:"Step 1: Install Python dependencies",id:"step-1-install-python-dependencies",level:3},{value:"Step 2: Create a <code>app.py</code> file",id:"step-2-create-a-apppy-file",level:3},{value:"Step 3: Add the code!",id:"step-3-add-the-code",level:3},{value:"Step 4: Run the Prem services",id:"step-4-run-the-prem-services",level:3},{value:"Step 5: Run the app",id:"step-5-run-the-app",level:3}],c={toc:p},h="wrapper";function m(e){let{components:t,...i}=e;return(0,r.kt)(h,(0,a.Z)({},c,i,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"Build a chatbot that talks to your data with ",(0,r.kt)("a",{parentName:"p",href:"https://premai.io"},"Prem")," using ",(0,r.kt)("inlineCode",{parentName:"p"},"LangChain"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"Chainlit"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"Chroma")," Vector Store and ",(0,r.kt)("inlineCode",{parentName:"p"},"Vicuna 7B")," model, self-hosted on your MacOS laptop."),(0,r.kt)("p",null,(0,r.kt)("img",{alt:"ChainLit x Langchain Screenshot",src:n(27025).Z,width:"1100",height:"716"})),(0,r.kt)("h3",{id:"what-is-chainlit"},"What is ChainLit?"),(0,r.kt)("p",null,"Chainlit lets you create ChatGPT-like UIs on top of any Python code in minutes!"),(0,r.kt)("h3",{id:"what-is-langchain"},"What is Langchain?"),(0,r.kt)("p",null,"LangChain is a framework designed to simplify the creation of applications using large language models (LLMs)."),(0,r.kt)("h3",{id:"what-is-prem"},"What is Prem?"),(0,r.kt)("p",null,"Prem is a self-hosted AI platform that allows you to test and deploy open-source AI models on your own infrastructure. Prem is open-source and free to use. You can learn more about Prem ",(0,r.kt)("a",{parentName:"p",href:"https://premai.io"},"here"),"."),(0,r.kt)("h2",{id:"talk-to-your-data-with-prem"},"Talk to your data with Prem"),(0,r.kt)("p",null,"We\u2019re going to build an chatbot QA app. We\u2019ll learn how to:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Upload a document"),(0,r.kt)("li",{parentName:"ul"},"Create vector embeddings from a file"),(0,r.kt)("li",{parentName:"ul"},"Create a chatbot app with the ability to display sources used to generate an answer")),(0,r.kt)("p",null,"For this tutorial we are going to use:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://chainlit.io"},"ChainLit")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("a",{parentName:"li",href:"https://docs.langchain.com/docs"},"Langchain")),(0,r.kt)("li",{parentName:"ul"},"Vicuna 7B model hosted on ",(0,r.kt)("a",{parentName:"li",href:"https://premai.io"},"Prem App"))),(0,r.kt)("h3",{id:"step-1-install-python-dependencies"},"Step 1: Install Python dependencies"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"pip install chainlit langchain chromadb tiktoken\n")),(0,r.kt)("h3",{id:"step-2-create-a-apppy-file"},"Step 2: Create a ",(0,r.kt)("inlineCode",{parentName:"h3"},"app.py")," file"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"touch app.py\n")),(0,r.kt)("h3",{id:"step-3-add-the-code"},"Step 3: Add the code!"),(0,r.kt)("p",null,"Please edit accordingly the Vicuna model and the sentence transformers for Embeddings. In this example it's ",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:8111/v1")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"http://localhost:8444/v1")," respectively."),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-python"},'from langchain.embeddings.openai import OpenAIEmbeddings\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores.chroma import Chroma\nfrom langchain.chains import RetrievalQAWithSourcesChain, LLMChain\nfrom langchain.chains.question_answering import load_qa_chain\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.prompts import PromptTemplate\nfrom langchain.prompts.chat import (\n    ChatPromptTemplate,\n    SystemMessagePromptTemplate,\n    HumanMessagePromptTemplate,\n)\nimport os\nimport chainlit as cl\n\nos.environ["OPENAI_API_KEY"] = "XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX"\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=100, chunk_overlap=10)\n\nsystem_template = """Use the following pieces of context to answer the users question.\nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.\nALWAYS return a "SOURCES" part in your answer.\nThe "SOURCES" part should be a reference to the source of the document from which you got your answer.\n\nExample of your response should be:\n\n`\nThe answer is foo\nSOURCES: xyz\n`\n\nBegin!\n----------------\n{summaries}"""\nmessages = [\n    SystemMessagePromptTemplate.from_template(system_template),\n    HumanMessagePromptTemplate.from_template("{question}"),\n]\nprompt = ChatPromptTemplate.from_messages(messages)\nchain_type_kwargs = {"prompt": prompt}\n\n@cl.langchain_factory(use_async=True)\nasync def init():\n    files = None\n\n    # Wait for the user to upload a file\n    while files == None:\n        files = await cl.AskFileMessage(\n            content="Please upload a text file to begin!", accept=["text/plain"]\n        ).send()\n\n    file = files[0]\n\n    msg = cl.Message(content=f"Processing `{file.name}`...")\n    await msg.send()\n\n    # Decode the file\n    text = file.content.decode("utf-8")\n\n    # Split the text into chunks\n    texts = text_splitter.split_text(text)\n\n    # Create a metadata for each chunk\n    metadatas = [{"source": f"{i}-pl"} for i in range(len(texts))]\n\n    # Create a Chroma vector store\n    embeddings = OpenAIEmbeddings(\n        openai_api_base="http://localhost:8444/v1"\n    )\n    docsearch = await cl.make_async(Chroma.from_texts)(\n        texts, embeddings, metadatas=metadatas\n    )\n    # Create a chain that uses the Chroma vector store\n    chat = ChatOpenAI(\n        temperature=0,\n        streaming=True,\n        max_tokens=128,\n        openai_api_base="http://localhost:8111/v1"\n    )\n\n    chain = RetrievalQAWithSourcesChain.from_chain_type(\n        chat, chain_type="stuff", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\n    chain.reduce_k_below_max_tokens = True\n    chain.max_tokens_limit = 128\n\n    # Save the metadata and texts in the user session\n    cl.user_session.set("metadatas", metadatas)\n    cl.user_session.set("texts", texts)\n\n    # Let the user know that the system is ready\n    await msg.update(content=f"`{file.name}` processed. You can now ask questions!")\n\n    return chain\n\n\n@cl.langchain_postprocess\nasync def process_response(res):\n    answer = res["answer"]\n    sources = res["sources"].strip()\n    source_elements = []\n\n    # Get the metadata and texts from the user session\n    metadatas = cl.user_session.get("metadatas")\n    all_sources = [m["source"] for m in metadatas]\n    texts = cl.user_session.get("texts")\n\n    if sources:\n        found_sources = []\n\n        # Add the sources to the message\n        for source in sources.split(","):\n            source_name = source.strip().replace(".", "")\n            # Get the index of the source\n            try:\n                index = all_sources.index(source_name)\n            except ValueError:\n                continue\n            text = texts[index]\n            found_sources.append(source_name)\n            # Create the text element referenced in the message\n            source_elements.append(cl.Text(content=text, name=source_name))\n\n        if found_sources:\n            answer += f"\\nSources: {\', \'.join(found_sources)}"\n        else:\n            answer += "\\nNo sources found"\n\n    await cl.Message(content=answer, elements=source_elements).send()\n')),(0,r.kt)("h3",{id:"step-4-run-the-prem-services"},"Step 4: Run the Prem services"),(0,r.kt)("p",null,"In the Prem App running on your laptop, start the following services clicking on ",(0,r.kt)("inlineCode",{parentName:"p"},"Open")," button:"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"Vicuna 7B Q4")," under ",(0,r.kt)("inlineCode",{parentName:"li"},"Chat")),(0,r.kt)("li",{parentName:"ul"},(0,r.kt)("inlineCode",{parentName:"li"},"All MiniLM L6 v2")," under ",(0,r.kt)("inlineCode",{parentName:"li"},"Embeddings"))),(0,r.kt)("h3",{id:"step-5-run-the-app"},"Step 5: Run the app"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-bash"},"chainlit run app.py\n")),(0,r.kt)("p",null,"You can then upload any ",(0,r.kt)("inlineCode",{parentName:"p"},".txt")," file to the UI and ask questions about it. If you are using ",(0,r.kt)("a",{parentName:"p",href:"https://github.com/hwchase17/langchain/blob/master/docs/extras/modules/state_of_the_union.txt"},(0,r.kt)("inlineCode",{parentName:"a"},"state_of_the_union.txt"))," you can ask questions like ",(0,r.kt)("inlineCode",{parentName:"p"},"What did the president say about Ketanji Brown Jackson?")))}m.isMDXComponent=!0},27025:(e,t,n)=>{n.d(t,{Z:()=>a});const a=n.p+"assets/images/chainlit-langchain-60bd0afe8bcd5f8f975edbc2bd902087.gif"}}]);