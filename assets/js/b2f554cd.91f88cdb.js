"use strict";(self.webpackChunkprem_docs=self.webpackChunkprem_docs||[]).push([[1477],{30010:e=>{e.exports=JSON.parse('{"blogPosts":[{"id":"chainlit-langchain-prem","metadata":{"permalink":"/blog/chainlit-langchain-prem","editUrl":"https://github.com/premAI-io/dev-portal/blog/2023-07-05-chainlit-langchain-qa/index.md","source":"@site/blog/2023-07-05-chainlit-langchain-qa/index.md","title":"Talk to your Data with ChainLit and Langchain","description":"Build a chatbot that talks to your data with Prem using LangChain, Chainlit, Chroma Vector Store and Vicuna 7B model, self-hosted on your MacOS laptop.","date":"2023-07-05T00:00:00.000Z","formattedDate":"July 5, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"langchain","permalink":"/blog/tags/langchain"},{"label":"chainlit","permalink":"/blog/tags/chainlit"},{"label":"vicuna-7b","permalink":"/blog/tags/vicuna-7-b"},{"label":"chroma","permalink":"/blog/tags/chroma"},{"label":"vector-store","permalink":"/blog/tags/vector-store"}],"readingTime":3.54,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"},{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"chainlit-langchain-prem","title":"Talk to your Data with ChainLit and Langchain","authors":["tiero","filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","langchain","chainlit","vicuna-7b","chroma","vector-store"]},"nextItem":{"title":"Serving Falcon 7B Instruct with FastAPI and Docker","permalink":"/blog/serving-falcon-7b-fastapi-docker"}},"content":"Build a chatbot that talks to your data with [Prem](https://premai.io) using `LangChain`, `Chainlit`, `Chroma` Vector Store and `Vicuna 7B` model, self-hosted on your MacOS laptop.\\n\\n![ChainLit x Langchain Screenshot](./chainlit-langchain.gif)\\n\\n\\n\x3c!--truncate--\x3e\\n\\n### What is ChainLit?\\n\\nChainlit lets you create ChatGPT-like UIs on top of any Python code in minutes!\\n\\n### What is Langchain?\\n\\nLangChain is a framework designed to simplify the creation of applications using large language models (LLMs).\\n\\n### What is Prem?\\n\\nPrem is a self-hosted AI platform that allows you to test and deploy open-source AI models on your own infrastructure. Prem is open-source and free to use. You can learn more about Prem [here](https://premai.io).\\n\\n\\n## Talk to your data with Prem\\n\\nWe\u2019re going to build an chatbot QA app. We\u2019ll learn how to:\\n\\n- Upload a document\\n- Create vector embeddings from a file\\n- Create a chatbot app with the ability to display sources used to generate an answer\\n\\n\\nFor this tutorial we are going to use:\\n\\n- [ChainLit](https://chainlit.io)\\n- [Langchain](https://docs.langchain.com/docs)\\n- Vicuna 7B model hosted on [Prem App](https://premai.io)\\n\\n### Step 1: Install Python dependencies\\n\\n```bash\\npip install chainlit langchain chromadb tiktoken\\n```\\n\\n### Step 2: Create a `app.py` file\\n\\n```bash\\ntouch app.py\\n```\\n\\n### Step 3: Add the code!\\n\\nPlease edit accordingly the Vicuna model and the sentence transformers for Embeddings. In this example it\'s `http://localhost:8111/v1` and `http://localhost:8444/v1` respectively.\\n\\n```python\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\\nfrom langchain.vectorstores.chroma import Chroma\\nfrom langchain.chains import RetrievalQAWithSourcesChain, LLMChain\\nfrom langchain.chains.question_answering import load_qa_chain\\nfrom langchain.chains.qa_with_sources import load_qa_with_sources_chain\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nimport os\\nimport chainlit as cl\\n\\nos.environ[\\"OPENAI_API_KEY\\"] = \\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\\"\\n\\ntext_splitter = RecursiveCharacterTextSplitter(\\n    chunk_size=100, chunk_overlap=10)\\n\\nsystem_template = \\"\\"\\"Use the following pieces of context to answer the users question.\\nIf you don\'t know the answer, just say that you don\'t know, don\'t try to make up an answer.\\nALWAYS return a \\"SOURCES\\" part in your answer.\\nThe \\"SOURCES\\" part should be a reference to the source of the document from which you got your answer.\\n\\nExample of your response should be:\\n\\n`\\nThe answer is foo\\nSOURCES: xyz\\n`\\n\\nBegin!\\n----------------\\n{summaries}\\"\\"\\"\\nmessages = [\\n    SystemMessagePromptTemplate.from_template(system_template),\\n    HumanMessagePromptTemplate.from_template(\\"{question}\\"),\\n]\\nprompt = ChatPromptTemplate.from_messages(messages)\\nchain_type_kwargs = {\\"prompt\\": prompt}\\n\\n@cl.langchain_factory(use_async=True)\\nasync def init():\\n    files = None\\n\\n    # Wait for the user to upload a file\\n    while files == None:\\n        files = await cl.AskFileMessage(\\n            content=\\"Please upload a text file to begin!\\", accept=[\\"text/plain\\"]\\n        ).send()\\n\\n    file = files[0]\\n\\n    msg = cl.Message(content=f\\"Processing `{file.name}`...\\")\\n    await msg.send()\\n\\n    # Decode the file\\n    text = file.content.decode(\\"utf-8\\")\\n\\n    # Split the text into chunks\\n    texts = text_splitter.split_text(text)\\n\\n    # Create a metadata for each chunk\\n    metadatas = [{\\"source\\": f\\"{i}-pl\\"} for i in range(len(texts))]\\n\\n    # Create a Chroma vector store\\n    embeddings = OpenAIEmbeddings(\\n        openai_api_base=\\"http://localhost:8444/v1\\"\\n    )\\n    docsearch = await cl.make_async(Chroma.from_texts)(\\n        texts, embeddings, metadatas=metadatas\\n    )\\n    # Create a chain that uses the Chroma vector store\\n    chat = ChatOpenAI(\\n        temperature=0,\\n        streaming=True,\\n        max_tokens=128,\\n        openai_api_base=\\"http://localhost:8111/v1\\"\\n    )\\n\\n    chain = RetrievalQAWithSourcesChain.from_chain_type(\\n        chat, chain_type=\\"stuff\\", retriever=docsearch.as_retriever(), chain_type_kwargs=chain_type_kwargs)\\n    chain.reduce_k_below_max_tokens = True\\n    chain.max_tokens_limit = 128\\n\\n    # Save the metadata and texts in the user session\\n    cl.user_session.set(\\"metadatas\\", metadatas)\\n    cl.user_session.set(\\"texts\\", texts)\\n\\n    # Let the user know that the system is ready\\n    await msg.update(content=f\\"`{file.name}` processed. You can now ask questions!\\")\\n\\n    return chain\\n\\n\\n@cl.langchain_postprocess\\nasync def process_response(res):\\n    answer = res[\\"answer\\"]\\n    sources = res[\\"sources\\"].strip()\\n    source_elements = []\\n\\n    # Get the metadata and texts from the user session\\n    metadatas = cl.user_session.get(\\"metadatas\\")\\n    all_sources = [m[\\"source\\"] for m in metadatas]\\n    texts = cl.user_session.get(\\"texts\\")\\n\\n    if sources:\\n        found_sources = []\\n\\n        # Add the sources to the message\\n        for source in sources.split(\\",\\"):\\n            source_name = source.strip().replace(\\".\\", \\"\\")\\n            # Get the index of the source\\n            try:\\n                index = all_sources.index(source_name)\\n            except ValueError:\\n                continue\\n            text = texts[index]\\n            found_sources.append(source_name)\\n            # Create the text element referenced in the message\\n            source_elements.append(cl.Text(content=text, name=source_name))\\n\\n        if found_sources:\\n            answer += f\\"\\\\nSources: {\', \'.join(found_sources)}\\"\\n        else:\\n            answer += \\"\\\\nNo sources found\\"\\n\\n    await cl.Message(content=answer, elements=source_elements).send()\\n```\\n\\n### Step 4: Run the Prem services\\n\\nIn the Prem App running on your laptop, start the following services clicking on `Open` button:\\n\\n- `Vicuna 7B Q4` under `Chat`\\n- `All MiniLM L6 v2` under `Embeddings`\\n\\n\\n### Step 5: Run the app\\n\\n```bash\\nchainlit run app.py\\n```\\n\\nYou can then upload any `.txt` file to the UI and ask questions about it. If you are using [`state_of_the_union.txt`](https://github.com/hwchase17/langchain/blob/master/docs/extras/modules/state_of_the_union.txt) you can ask questions like `What did the president say about Ketanji Brown Jackson?`"},{"id":"serving-falcon-7b-fastapi-docker","metadata":{"permalink":"/blog/serving-falcon-7b-fastapi-docker","editUrl":"https://github.com/premAI-io/dev-portal/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md","source":"@site/blog/2023-07-03-serve-falcon-with-fastapi-and-docker/index.md","title":"Serving Falcon 7B Instruct with FastAPI and Docker","description":"Prem Banner","date":"2023-07-03T00:00:00.000Z","formattedDate":"July 3, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"fastapi","permalink":"/blog/tags/fastapi"},{"label":"docker","permalink":"/blog/tags/docker"},{"label":"falcon-7b","permalink":"/blog/tags/falcon-7-b"}],"readingTime":8.32,"hasTruncateMarker":false,"authors":[{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"serving-falcon-7b-fastapi-docker","title":"Serving Falcon 7B Instruct with FastAPI and Docker","authors":["filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","fastapi","docker","falcon-7b"]},"prevItem":{"title":"Talk to your Data with ChainLit and Langchain","permalink":"/blog/chainlit-langchain-prem"},"nextItem":{"title":"Build a Perplexity AI clone on Prem","permalink":"/blog/perplexity-ai-self-hosted"}},"content":"![Prem Banner](./banner.jpg)\\n\\n<head>\\n  <meta name=\\"twitter:image\\" content=\\"./banner.jpg\\"/>\\n</head>\\n\\nIn this tutorial, we will walk you through the process of serving the Falcon 7B Instruction model using FastAPI and Docker. The complete code for this tutorial is available on [GitHub](https://github.com/premAI-io/llm-fastapi-docker-template).\\n\\n> NOTE: in order to run Falcon 7B Instruct model you will need a GPU with at least 16GiB of VRAM. You can use a [Paperspace Cloud](https://www.paperspace.com/gpu-cloud) virtual server or any other cloud provider or your own server with a NVIDIA GPU.\\n\\n##### If you want to just use the model for inference directly, you can use our pre-built docker image like this:\\n\\n```bash\\ndocker run --gpus all -p 8000:8000 ghcr.io/premai-io/chat-falcon-7b-instruct-gpu:latest\\n```\\nThis will ensure that the container has access to the GPU and will expose the API on port 8000. [Learn more](https://github.com/premAI-io/prem-registry/blob/dev/chat-falcon-7b-instruct/README.md).\\n\\n### Step 1: Setup the Python Server\\n\\nFirst, we need to create a requirements.txt file to list all the necessary dependencies. This file will include libraries such as FastAPI, uvicorn, pytest, requests, tqdm, httpx, python-dotenv, tenacity, einops, sentencepiece, accelerate, and xformers.\\n\\n#### 1. Create `requirements.txt` file.\\n\\nCreate a `requirements.txt` file with the following dependencies:\\n\\n```txt\\nfastapi==0.95.0\\nuvicorn==0.21.1\\npytest==7.2.2\\nrequests==2.28.2\\ntqdm==4.65.0\\nhttpx==0.23.3\\npython-dotenv==1.0.0\\ntenacity==8.2.2\\neinops==0.6.1\\nsentencepiece==0.1.99\\naccelerate>=0.16.0,<1\\nxformers==0.0.20\\n```\\n\\n### Step 2: Expose Falcon 7B Instruct with FastAPI\\n\\nNext, we will create a `models.py` file to define the model class that will be used to serve the Falcon 7B Instruction model. We will use the `transformers` library to fetch the model from the HuggingFace Hub.\\n\\nWe will also need a `utils.py` file to define the stopping criteria for the Falcon model. This criteria is used to signal the model when to stop generating new tokens.\\n\\nFinally, we will create a `routes.py` file to define the endpoints that our FastAPI web server will handle. This file will include the logic for generating responses and handling exceptions.\\n\\n#### 1. Create `models.py` file.\\n\\n```python\\nimport os\\nimport torch\\n\\nfrom typing import List\\nfrom transformers import AutoTokenizer, Pipeline, pipeline\\n\\nclass FalconBasedModel(object):\\n    model = None\\n    stopping_criteria = None\\n\\n    @classmethod\\n    def generate(\\n        cls,\\n        messages: list,\\n        temperature: float = 0.9,\\n        top_p: float = 0.9,\\n        n: int = 1,\\n        stream: bool = False,\\n        max_tokens: int = 256,\\n        stop: str = \\"\\",\\n        **kwargs,\\n    ) -> List:\\n        message = messages[-1][\\"content\\"]\\n        return [\\n            cls.model(\\n                message,\\n                max_length=max_tokens,\\n                num_return_sequences=n,\\n                temperature=temperature,\\n                top_p=top_p,\\n                eos_token_id=cls.tokenizer.eos_token_id,\\n                return_full_text=kwargs.get(\\"return_full_text\\", False),\\n                do_sample=kwargs.get(\\"do_sample\\", True),\\n                stop_sequence=stop[0] if stop else None,\\n                stopping_criteria=cls.stopping_criteria(stop, message, cls.tokenizer),\\n            )[0][\\"generated_text\\"]\\n        ]\\n\\n    @classmethod\\n    def get_model(cls) -> Pipeline:\\n        if cls.model is None:\\n            cls.tokenizer = AutoTokenizer.from_pretrained(\\n                os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n                trust_remote_code=True,\\n            )\\n            cls.model = pipeline(\\n                tokenizer=cls.tokenizer,\\n                model=os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n                torch_dtype=torch.bfloat16,\\n                trust_remote_code=True,\\n                device_map=os.getenv(\\"DEVICE\\", \\"auto\\"),\\n            )\\n        cls.stopping_criteria = FalconStoppingCriteria\\n        return cls.model\\n\\n```\\n\\nIn the provided code, a class named `FalconBasedModel` is used to encapsulate the functionality related to the Falcon 7B Instruction model. This class-based approach has several advantages:\\n\\n1. **Encapsulation**: By using a class, we can bundle together the model, its tokenizer, and its stopping criteria into a single unit. This makes the code more organized and easier to understand. It also allows us to hide the internal details of how the model works, exposing only the methods that are necessary for interacting with it.\\n\\n2. **State Preservation**: Class methods can access and modify the state of an instance of the class. In this case, the `FalconBasedModel` class maintains the state of the model and its tokenizer. This is useful because it allows us to load the model and tokenizer only once, when the `get_model` method is first called, and then reuse them for subsequent calls to the `generate` method. This can significantly improve performance, as loading a model and tokenizer can be computationally expensive operations.\\n\\n#### 2. Create `utils.py` file.\\n\\nLike other generative AI models, Falcon requires a stopping criteria to determine when to cease generating new tokens. We will use a straightforward stopping criteria that checks if the target sequence is present in the generated text. The default value is set to \'User:\', but developers can provide a custom target sequence through APIs.\\n\\n```python\\n\\nfrom typing import List\\n\\nfrom transformers import StoppingCriteria\\n\\nclass FalconStoppingCriteria(StoppingCriteria):\\n    def __init__(self, target_sequences: List[str], prompt, tokenizer) -> None:\\n        self.target_sequences = target_sequences\\n        self.prompt = prompt\\n        self.tokenizer = tokenizer\\n\\n    def __call__(self, input_ids, scores, **kwargs) -> bool:\\n        if not self.target_sequences:\\n            return False\\n        # Get the generated text as a string\\n        generated_text = self.tokenizer.decode(input_ids[0])\\n        generated_text = generated_text.replace(self.prompt, \\"\\")\\n        # Check if the target sequence appears in the generated text\\n        return any(\\n            target_sequence in generated_text\\n            for target_sequence in self.target_sequences\\n        )\\n\\n    def __len__(self) -> int:\\n        return len(self.target_sequences)\\n\\n    def __iter__(self):\\n        yield self\\n```\\n\\n#### 3. Create `routes.py` file.\\n\\nNext, we\'ll define the endpoints that our FastAPI web server will handle.\\n\\n```python\\nimport json\\nimport os\\nimport uuid\\nfrom datetime import datetime as dt\\nfrom typing import Any, Dict, Generator, List, Optional, Union\\n\\nfrom fastapi import APIRouter, HTTPException\\nfrom fastapi.responses import StreamingResponse\\nfrom pydantic import BaseModel\\n\\nfrom models import FalconBasedModel as model\\n\\n\\nclass ChatCompletionInput(BaseModel):\\n    model: str\\n    messages: List[dict]\\n    temperature: float = 1.0\\n    top_p: float = 1.0\\n    n: int = 1\\n    stream: bool = False\\n    stop: Optional[Union[str, List[str]]] = [\\"User:\\"]\\n    max_tokens: int = 64\\n    presence_penalty: float = 0.0\\n    frequence_penalty: float = 0.0\\n    logit_bias: Optional[dict] = {}\\n    user: str = \\"\\"\\n\\n\\nclass ChatCompletionResponse(BaseModel):\\n    id: str = uuid.uuid4()\\n    model: str\\n    object: str = \\"chat.completion\\"\\n    created: int = int(dt.now().timestamp())\\n    choices: List[dict]\\n    usage: dict = {\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0}\\n\\n\\nrouter = APIRouter()\\n\\n\\nasync def generate_chunk_based_response(body, text) -> Generator[str, Any, None]:\\n    yield \\"event: completion\\\\ndata: \\" + json.dumps(\\n        {\\n            \\"id\\": str(uuid.uuid4()),\\n            \\"model\\": body.model,\\n            \\"object\\": \\"chat.completion\\",\\n            \\"choices\\": [\\n                {\\n                    \\"role\\": \\"assistant\\",\\n                    \\"index\\": 1,\\n                    \\"delta\\": {\\"role\\": \\"assistant\\", \\"content\\": text},\\n                    \\"finish_reason\\": \\"stop\\",\\n                }\\n            ],\\n            \\"usage\\": {\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0},\\n        }\\n    ) + \\"\\\\n\\\\n\\"\\n    yield \\"event: done\\\\ndata: [DONE]\\\\n\\\\n\\"\\n\\n\\n@router.post(\\"/chat/completions\\", response_model=ChatCompletionResponse)\\nasync def chat_completions(body: ChatCompletionInput) -> Dict[str, Any]:\\n    try:\\n        predictions = model.generate(\\n            messages=body.messages,\\n            temperature=body.temperature,\\n            top_p=body.top_p,\\n            n=body.n,\\n            stream=body.stream,\\n            max_tokens=body.max_tokens,\\n            stop=body.stop,\\n        )\\n        if body.stream:\\n            return StreamingResponse(\\n                generate_chunk_based_response(body, predictions[0]),\\n                media_type=\\"text/event-stream\\",\\n            )\\n        return ChatCompletionResponse(\\n            id=str(uuid.uuid4()),\\n            model=os.getenv(\\"MODEL_ID\\", \\"tiiuae/falcon-7b-instruct\\"),\\n            object=\\"chat.completion\\",\\n            created=int(dt.now().timestamp()),\\n            choices=[\\n                {\\n                    \\"role\\": \\"assistant\\",\\n                    \\"index\\": idx,\\n                    \\"message\\": {\\"role\\": \\"assistant\\", \\"content\\": text},\\n                    \\"finish_reason\\": \\"stop\\",\\n                }\\n                for idx, text in enumerate(predictions)\\n            ],\\n            usage={\\"prompt_tokens\\": 0, \\"completion_tokens\\": 0, \\"total_tokens\\": 0},\\n        )\\n    except ValueError as error:\\n        raise HTTPException(\\n            status_code=400,\\n            detail={\\"message\\": str(error)},\\n        )\\n\\n```\\n\\n#### 4. Create `main.py` file.\\n\\nThe `main.py` file is the entry point for our FastAPI application. It is responsible for setting up the application and starting the server.\\n\\nOne important aspect of this file is the `create_start_app_handler` function. This function is designed to be called when the FastAPI application starts up. It creates a function, `start_app`, that is responsible for loading the Falcon 7B Instruction model into memory. This is done by calling the `get_model` method of the `FalconBasedModel` class.\\n\\nThe reason we load the model into memory at startup is to improve the performance of our application. Loading a model is a time-consuming operation. If we were to load the model every time we needed to use it, it would significantly slow down our application. By loading the model at startup, we ensure that it\'s done only once, no matter how many requests our application needs to handle.\\n\\nThe `start_app` function is then returned and registered as a startup event handler for our FastAPI application. This means that FastAPI will automatically call this function when the application starts up, ensuring that our model is loaded and ready to use.\\n\\nThe rest of the `main.py` file is responsible for setting up the FastAPI application, including registering our API routes and setting up CORS (Cross-Origin Resource Sharing) middleware. Finally, if this file is run directly (i.e., it is the main module), it starts the FastAPI server using uvicorn.\\n\\n```python\\nimport logging\\nfrom typing import Callable\\n\\nimport uvicorn\\nfrom dotenv import load_dotenv\\nfrom fastapi import FastAPI\\nfrom fastapi.middleware.cors import CORSMiddleware\\nfrom routes import router as api_router\\n\\nfrom models import FalconBasedModel\\n\\ndef create_start_app_handler(app: FastAPI) -> Callable[[], None]:\\n    def start_app() -> None:\\n        FalconBasedModel.get_model()\\n\\n    return start_app\\n\\n\\ndef get_application() -> FastAPI:\\n    application = FastAPI(title=\\"prem-chat-falcon\\", debug=True, version=\\"0.0.1\\")\\n    application.include_router(api_router, prefix=\\"/v1\\")\\n    application.add_event_handler(\\"startup\\", create_start_app_handler(application))\\n    application.add_middleware(\\n        CORSMiddleware,\\n        allow_origins=[\\"*\\"],\\n        allow_credentials=True,\\n        allow_methods=[\\"*\\"],\\n        allow_headers=[\\"*\\"],\\n    )\\n    return application\\n\\n\\napp = get_application()\\n\\n\\nif __name__ == \\"__main__\\":\\n    uvicorn.run(\\"main:app\\", host=\\"0.0.0.0\\", port=8000)\\n\\n```\\n\\n### Step 3: Use Docker to build and run the application\\n\\nTo build and run the application, we will first create a `download.py` file. This script will be called at build time to download the model and cache it in the Docker image.\\n\\nNext, we will create a Dockerfile that uses the official image from HuggingFace, which includes all the necessary dependencies. This Dockerfile will define the steps to build our Docker image.\\n\\nTo avoid including any unused files in the build process, we will also create a `.dockerignore` file.\\n\\n#### 1. Create a `download.py` file.\\n\\nThe download script will be called at build time to download the model and cache it in the Docker image.\\n\\n```python\\nimport argparse\\nimport os\\n\\nimport torch\\nimport transformers\\nfrom tenacity import retry, stop_after_attempt, wait_fixed\\nfrom transformers import AutoTokenizer\\n\\nparser = argparse.ArgumentParser()\\nparser.add_argument(\\"--model\\", help=\\"Model to download\\")\\nargs = parser.parse_args()\\n\\nprint(f\\"Downloading model {args.model}\\")\\n\\n\\n@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\\ndef download_model() -> None:\\n    _ = AutoTokenizer.from_pretrained(args.model, trust_remote_code=True)\\n    _ = transformers.pipeline(\\n        model=args.model,\\n        torch_dtype=torch.bfloat16,\\n        trust_remote_code=True,\\n        device_map=os.getenv(\\"DEVICE\\", \\"auto\\"),\\n    )\\n\\n\\ndownload_model()\\n\\n```\\n\\n#### 2. Create a `Dockerfile`.\\n\\n```dockerfile\\nFROM huggingface/transformers-pytorch-gpu:4.28.1\\n\\nARG MODEL_ID\\n\\nWORKDIR /usr/src/app/\\n\\nCOPY requirements.txt ./\\n\\nRUN pip install --no-cache-dir -r ./requirements.txt --upgrade pip\\n\\nCOPY download.py .\\n\\nRUN python3 download.py --model $MODEL_ID\\n\\nCOPY . .\\n\\nENV MODEL_ID=$MODEL_ID\\n\\nCMD python3 main.py\\n```\\n\\n#### 4. Create a `.dockerignore` file.\\n\\n```dockerfile\\n.editorconfig\\n.gitattributes\\n.github\\n.gitignore\\n.gitlab-ci.yml\\n.idea\\n.pre-commit-config.yaml\\n.readthedocs.yml\\n.travis.yml\\nvenv\\n.git\\n./ml/models/\\n.bin\\n```\\n\\n### Step 4: Build and run the application\\n\\nFinally, we will build the Docker image using the `docker build` command and run it using the `docker run` command.\\n\\n#### 1. Build the Docker image.\\n\\n```bash\\ndocker build --file ./Dockerfile \\\\\\n    --build-arg=\\"MODEL_ID=tiiuae/falcon-7b-instruct\\" \\\\\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:latest \\\\\\n    --tag blog-post/chat-falcon-7b-instruct-gpu:0.0.1 \\\\\\n    .\\n```\\n\\n#### 2. Run the Docker image.\\n\\n```bash\\ndocker run --gpus all -p 8000:8000 blog-post/chat-falcon-7b-instruct-gpu:latest\\n```\\n\\n### Conclusion\\n\\nIn this tutorial, we have demonstrated how to serve the Falcon 7B Instruction model using FastAPI and Docker. This is a crucial first step in serving a model for production use cases. [Learn more](/docs/category/service-packaging/) about packaging your model and exposing it to the Prem Ecosystem to quickly get up and running with your AI initiatives."},{"id":"perplexity-ai-self-hosted","metadata":{"permalink":"/blog/perplexity-ai-self-hosted","editUrl":"https://github.com/premAI-io/dev-portal/blog/2023-07-01-perplexity-ai-self-hosted/index.md","source":"@site/blog/2023-07-01-perplexity-ai-self-hosted/index.md","title":"Build a Perplexity AI clone on Prem","description":"Build your own Perplexity AI clone with Prem using the Dolly v2 12B model, self-hosted on Paperspace Cloud virtual server.","date":"2023-07-01T00:00:00.000Z","formattedDate":"July 1, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"ai","permalink":"/blog/tags/ai"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"perplexity","permalink":"/blog/tags/perplexity"},{"label":"paperspace","permalink":"/blog/tags/paperspace"},{"label":"dolly","permalink":"/blog/tags/dolly"}],"readingTime":3.06,"hasTruncateMarker":true,"authors":[{"name":"Marco Argentieri","title":"Bitcoin wizard","url":"https://github.com/tiero","imageURL":"https://github.com/tiero.png","key":"tiero"}],"frontMatter":{"slug":"perplexity-ai-self-hosted","title":"Build a Perplexity AI clone on Prem","authors":["tiero"],"tags":["llm","ai","self-hosted","prem","open-source","perplexity","paperspace","dolly"]},"prevItem":{"title":"Serving Falcon 7B Instruct with FastAPI and Docker","permalink":"/blog/serving-falcon-7b-fastapi-docker"},"nextItem":{"title":"Hello Prem!","permalink":"/blog/hello-prem"}},"content":"<head>\\n  <meta name=\\"twitter:image\\" content=\\"./screenshot.png\\"/>\\n</head>\\n\\nBuild your own Perplexity AI clone with [Prem](https://premai.io) using the `Dolly v2 12B` model, self-hosted on [Paperspace Cloud](https://www.paperspace.com/gpu-cloud) virtual server.\\n\\n![Clarity AI Screenshot](./screenshot.png)\\n\x3c!--truncate--\x3e\\n\\n### What is Perplexity AI?\\n\\nPerplexity AI is a conversational search engine and chatbot that acts as a search engine that scans the internet to provide users with straightforward answers to their questions. It is a great tool for students, researchers, and anyone who wants to learn more about a topic.\\n\\n### What is Prem?\\n\\nPrem is a self-hosted AI platform that allows you to test and deploy open-source AI models on your own infrastructure. Prem is open-source and free to use. You can learn more about Prem [here](https://premai.io).\\n\\n## Build Perplexity AI with Prem\\n\\n### Overview\\n\\nFor this tutorial we are going to use the **fantastic** open-source frontend [Clarity AI](https://github.com/mckaywrigley/clarity-ai) built by [Mckay Wrigley](https://github.com/mckaywrigley). \ud83d\udc4f Kudos for building such a great tool!\\n\\nSince `ClarityAI` uses ChatGPT by OpenAI, the integration with Prem it\'s staightforward as we only need to change the API endpoint and use a random string as API key, to skip the authentication.\\n\\nAs infrastructure, we are going to use [Paperspace Cloud](https://www.paperspace.com/gpu-cloud). You can use any other cloud provider or your own server with a NVIDIA GPU.\\n\\n\\n### Step 1: Little tweaks to the Clarity AI app\\n\\n> \u2139\ufe0f **skip this step** You can use directly my own `clarity-ai` fork [github.com/tiero/clarity-ai](https://github.com/tiero/clarity-ai) that has the changes already applied.\\n\\n#### 1. Clone the app\\n\\nFirst, we need to clone the Clarity AI repository. For future reference, we are using the follwing commit hash [`5a33db1`](https://github.com/mckaywrigley/clarity-ai/commit/5a33db140d253f47da3f07ad1475938c14dfda45).\\n\\n```bash\\ngit clone https://github.com/mckaywrigley/clarity-ai\\n```\\nOpen the `clarity-ai` project with your editor of choice. I\'m using [Visual Studio Code](https://code.visualstudio.com/).\\n\\n\\n#### 2. Set a random API key\\n\\nOpen the `components/Search.tsx` file, at line 16 we need to pre-populate the `apiKey` state with a random string. \\n\\n```typescript\\nconst [apiKey, setApiKey] = useState<string>(\\"X\\".repeat(51));\\n```\\nThis is needed because we are not going to use the authentication system of OpenAI and Prem is currently exposing the endpoints without authentication.\\n\\n#### 3. Set the API endpoint\\n\\nOpen the `utils/answer.ts` file, at line 8 we need to change the API endpoint from OpenAI to be sourced from environment variable `NEXT_PUBLIC_API_URL`\\n\\n```typescript\\n`${process.env.NEXT_PUBLIC_API_URL}/v1/chat/completions`\\n```\\n\\nDone! \ud83c\udf89\\n\\n### Step 2: Install Prem on Paperspace\\n\\n\\nCreate a Paperspace account if you don\'t have one already, then login to the [Paperspace Console](https://console.paperspace.com/).\\n\\n#### 1. Create a machine \\n\\n- **Machine Type**: `P6000`, `V100-32G`, `A100`, `A100-80G`, `A5000`, `A6000`\\n- **GPU**: `NVIDIA GPU`\\n- **Machine OS**: `ML-in-a-Box`\\n- **Machine Storage**: `50 GB`\\n- **Memory**: min 24 GiB\\n\\n#### 2. Connect to the instance via SSH\\n\\n```bash\\nssh paperspace@<your-instance-ip>\\n```\\n\\n#### 3. Install Prem\\n\\n```bash\\nwget -q https://get.prem.ninja/install.sh -O install.sh; sudo bash ./install.sh\\n```\\nThis can take a while, so grab an espresso \u2615\ufe0f\\n\\n#### 4. Check the app\\n\\nVisit the following URL in your browser: `http://<your-instance-ip>:8000` to confirm the Prem App is up and running.\\n\\n### Step 3: Download & Run the model \\n\\nFrom the Prem App, select the `Dolly v2 12B` model and click on the **dowload** icon.\\nOnce the model is downloaded, click **Open** button. This will start the container and open the chat UI. At this point we don\'t need the embedded user interface, so we can close it.\\n\\n### Step 4: Run the app\\n\\nNow back to the frontend, let\'s run it locally and connect to our Prem instance.\\n\\n#### 1. Set the right environment variable \\n\\n```bash\\nexport NEXT_PUBLIC_API_URL=http://<your-instance-ip>:8000\\n```\\n\\n#### 2. Install the dependencies\\n\\n```bash\\nnpm install\\n```\\n\\n#### 3. Run the frontend\\n\\n```bash\\nnpm run dev\\n```\\n\\n\\n### Enjoy!\\n\\nVisit the following URL in your browser: `http://localhost:3000` to start using your own Perplexity AI clone!"},{"id":"hello-prem","metadata":{"permalink":"/blog/hello-prem","editUrl":"https://github.com/premAI-io/dev-portal/blog/2023-06-26-welcome/index.md","source":"@site/blog/2023-06-26-welcome/index.md","title":"Hello Prem!","description":"Prem Banner","date":"2023-06-26T00:00:00.000Z","formattedDate":"June 26, 2023","tags":[{"label":"llm","permalink":"/blog/tags/llm"},{"label":"self-hosted","permalink":"/blog/tags/self-hosted"},{"label":"prem","permalink":"/blog/tags/prem"},{"label":"open-source","permalink":"/blog/tags/open-source"},{"label":"welcome","permalink":"/blog/tags/welcome"}],"readingTime":0.33,"hasTruncateMarker":false,"authors":[{"name":"Filippo Pedrazzini","title":"Core contributor @ PremAI","url":"https://github.com/filopedraz","imageURL":"https://github.com/filopedraz.png","key":"filippopedrazzinfp"}],"frontMatter":{"slug":"hello-prem","title":"Hello Prem!","authors":["filippopedrazzinfp"],"tags":["llm","self-hosted","prem","open-source","welcome"]},"prevItem":{"title":"Build a Perplexity AI clone on Prem","permalink":"/blog/perplexity-ai-self-hosted"}},"content":"![Prem Banner](./banner.png)\\n\\nHello, I am Filippo and I am currently contributing to Prem.\\n\\nWelcome to Prem!\\n\\n- Be part of the community [joining our Discord](https://discord.com/invite/kpKk6vYVAn).\\n- To stay in touch [follow us on Twitter](https://twitter.com/premai_io).\\n- To report bugs or ask for support [open an issue on the Github repository](https://github.com/premAI-io/prem-app).\\n\\nWe just released the first version of our Developer Portal. You can check it out at https://dev.premai.io"}]}')}}]);